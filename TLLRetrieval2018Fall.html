<!DOCTYPE HTML>
<html>
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Totally-Look-Like Retrieval</title>
    <link rel="stylesheet" href="http://yui.yahooapis.com/pure/0.5.0/pure-min.css">
    <link rel="stylesheet" type="text/css" href="content/bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="content/site.css" />
    <link rel="stylesheet" type="text/css" href="app/content/general_project.css" />

    <script src="scripts/modernizr-2.6.2.js"></script>
</head>

<body style="align-self:center;margin-left:10%;margin-right:10%;margin-top:5%">
    <!-- header -->
    <div style="text-align:center">
    <h1 style="align-content:center"><strong>Totally-Look-Like Retrieval</strong></h1>
    <h4 style="align-content:center"> Fall 2018</h4>
    <h4 style="align-content:center"><u>Aiyu Cui</u></h4>
    <h4 style="align-content:center">Computer Science, University of Illinois at Urbana-Champaign</h4>
       


    
    <!---->
    <div class="container" style="align-content:center">
        <!-- navbar-collapse start-->
        <div id="nav-menu" class="navbar-collapse collapse" role="navigation"  >
            <ul class="nav navbar-nav">
                <li>
                    <a href="#introduction"><span class="glyphicon glyphicon-file" aria-hidden="true"></span>Introduction</a>
                </li>
                <li>
                    <a href="#review"><span class="glyphicon glyphicon-file" aria-hidden="true"></span>Literature Review</a>
                </li>
                 <li>
                    <a href="#dataset"><span class="glyphicon glyphicon-file" aria-hidden="true"></span>Dataset</a>
                </li>
                <li>
                    <a href="#Baseline"><span class="glyphicon glyphicon-file" aria-hidden="true"></span>Retrieval Baseline</a>
                </li>
                <li>
                    <a href="#Conditional Retrieval"><span class="glyphicon glyphicon-file" aria-hidden="true"></span>Conditional Retrieval</a>
                </li>
                <li>
                    <a href="#discussion"><span class="glyphicon glyphicon-file" aria-hidden="true"></span>Discussion</a>
                </li>

                <li>
                    <a href="#References"><span class="glyphicon glyphicon-file" aria-hidden="true"></span>References</a>
                </li>
            </ul>
        </div>
    </div>
    </div>


    <!-- navbar-collapse end-->
    <ul>
        <li>
            <section id="introduction">
                <div class="section_title">
                    <h2><strong>Introduction</strong></h2>
                    <div class="section_body">
                        <p>Similarity Retrieval has been a hot topic in Computer Vision field for decades. Here we will focus on a non-trivial similarity retrieval, in which images are retrieved through a particular visual concepts based similarity rather than through a category-based similarity, as shown in Figure 0. 
<a href=" https://sites.google.com/view/totally-looks-like-dataset
">Totally-Look-Like Dataset [1]</a> is a new dataset first introduced in ACCV 2018 and this project will be evaluated based on Totally-Look-Like Dataset (TLL). </p>
                        <p>In this work, we first reimplement and adjust the TLL baseline. Then, we noticed that the baseline-like methods (using deep net feature directly) can retreve the category-based similarity well, which is sentitive to the between-class difference of training set but not sensitive in general visual concept based similarity that we are interested. </p>
                        <p>Thus, on top of the baseline-like method, we tried several ways to retrieve image similarity with respect to different visual concepts. With inspiration from both 
                        <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Veit_Conditional_Similarity_Networks_CVPR_2017_paper.pdf">Conditional Similarity Network (CSN) [2]</a> and 
                        <a href="https://arxiv.org/abs/1711.08389">Conditional Image-Text Embedding (CITE) [3]</a>, we train a neural network which can learn conditional embeddings by decomposing the original embeddings output from the deep net. Therefore, each conditional embedding could represent a different visual concept, from which we can later retrieve similarity. Besides to learn visual concepts via conditional embedding in a unsupervised way, we also discuss the potential to learn semantic concepts in a supervised way as what is introduced in <a href="http://people.csail.mit.edu/bzhou/publication/eccv18-IBD">Interpretable Basis Decomposition (IBD) [4]</a></p>
                    </div>
                    <div style='text-align: center;font-size: 80%'>
							<img src="images/TLLReport/TLLEx.PNG" style="width:60%;align-self:center" alt="" caption="aljda">
							<br />Figure 0: Example of TLL <a href=" https://sites.google.com/view/totally-looks-like-dataset
">[1]</a>
						</div>

                    <h3><strong>Brief Literature Review</strong></h3>
                    <div class="section_body">
                        This work is inspired by several work, here we are going to give a brief recall of each of them.
                        <h4>TLL</h4>
                        <p><a href=" https://sites.google.com/view/totally-looks-like-dataset
">Totally-Look-Like Dataset [1]</a> is a new dataset first introduced in ACCV 2018. The dataset contains visually similar pair-images, which are collected from a popular entertainment website. 
                        The proposed task of this dataset is image retrieval. each pair is divided into left and right set, such that one image goes to the left set and the other goes to the right. Given a query image from the left set, we want to find the most similar image from the right set or a subset of the right set. 
                        The proposed baseline method is to extract image features from the last activation layer from a deep net and use those features to compute cosine distance. The image retrieval is then considered as a latent space data point distance comparison problem.</p>

                        <h4>CSN and CITE</h4>
                        <p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Veit_Conditional_Similarity_Networks_CVPR_2017_paper.pdf">Conditional Similarity Network (CSN) [2]</a> and 
                        <a href="https://arxiv.org/abs/1711.08389">Conditional Image-Text Embedding Network(CITE) [3]</a> are two work that train a neural network which can learn how to decompose a union concept. </p>
                        <p>CSN tries to retrieve image similarity based on different visual concepts (e.g., clothing categories, clothing occassion and clothing colors.) It trains several notion branchs in parallel and M mask such that each unit on a mask can behave like a gate to decide whether use a certain notion to retrieve concept m. Triplet are sampled based on every concept m to perform a well-defined supervised learning.</p>
                        <p>CITE tries to learn group text with similar semantic meaning in the latent space in an implicit way by conditional or concept embedding layers. Rather use predefined clusters (like clothing categories and etc in previous), CITE introduces a conditional branch to learn a weight vectors which behaves like a gate to decide how much each output of the conditional layers participate in the final prediction. Both the conditional layers and the weight branch can be learned during training. </p>

                        <h4>IBD</h4>
                        <p> <a href="http://people.csail.mit.edu/bzhou/publication/eccv18-IBD">Interpretable Basis Decomposition (IBD) [4]</a> is a method proposed in ECCV 2018 to visualize the behaviors of neural network. It could decompose a prediction from a neural network in a form as the sum of comtribution of a set of base concepts. This is say, IBD provides a way to decompose a prediction into a set of basis vectors such that each base vector representing a basic visual concepts (objects, parts, color, textures, scenes, and etc). This is potential useful to us, if we want to decompose our embeddings and perform the retrieval based different visual concepts. </p>
                  </div>

                    <h3><strong>Dataset</strong></h3>
                    <div class="section_body">
                        Here, we briefly introduce the dataset used or potentially useful for this project.
                        <h4>Totally-Look-Like Dataset</h4>

                        <p>The main dataset we used to evaluate our method. We are going to perform the image retrieval on the TLL dataset. TLL dataset contains 6,016 pairs of images, such that each images similar in a way based on general visual concepts rather than category-based similarity, as shown in Figure 0. </p>
                        <p>There are two cleaned subsets: no_duplicate subset and no_face subset. As the dataset is collected from a popular entertainment website, there are many duplicates among the 2x6,016 images which hurts the accuracy of retrieval results. Thus no_duplicates is a subset of the full TLL dataset with all duplicates removed, which has 1,828 images remaining. Meanwhile, as noticed there are a large poriton of faces images in the TLL dataset, another subset, called no_face subset, is provided as all faces and duplicates are removed, which resulted in 1,622 pairs remaining. More details about the subset selection process refers to <a href=" https://sites.google.com/view/totally-looks-like-dataset
">Totally-Look-Like Dataset [1].</a></p>
						<p>Since TLL dataset is relatively small, we use the entire TLL dataset to test our models only.</p>

						<h4>ILSRVC 2012 [8]</h4>
						<p><a href="http://image-net.org/challenges/LSVRC/2012/">ILSRVC 2012 (ImageNet)</a> is the version of ImageNet we used. In this project, if we say a "pre-trained" model, we means a model pre-trained on this dataset. ILSRVC 2012 has 1,000 classes and 1.2 millions images for training and 150,000 images for testing. In this project, when we joint training on top of the pre-trained models, we random selected images from the training set. We use a 50,000 validation set for evaluation.</p>
                        <h4>The Behance Art Media Dataset (BAM) [5]</h4>
                        <p><a href='https://bam-dataset.org/'>The Behance Art Media Dataset (BAM) [5]</a> is art dataset built from Behance, a portfolio website of professional and commerical artist. Artwork in BAM includes painting, photography, graphic design, comic, oil plant, water color, and etc. 
                        As we noticed that there are many art work in TLL dataset, we choose BAM to finetune/joint-training/<a href='https://ieeexplore.ieee.org/abstract/document/8107520/'>LwF [6]</a> our ImageNet pre-trained models to get them familar with art features.</p>

                        <p>We only use the nine object labels (bicycle, bird, building, cars, cat, dog, flower, people and tree) in BAM. After preprocessing (random split with train:val:test=8:1:1), we have 60,353 images for training and 7,566 images for test for our experiments.</p>
                        <h4>BroDen</h4>
                        <p><a href='https://arxiv.org/abs/1704.05796'>BroDen [7]</a> dataset is the dataset IBD used to train the base vectors of visual concepts. It has fine-grained (pixel or image level) labels for visual concepts fall in categories objects, parts, scenes, textures, colors and materials.</p>
                  </div>
                </div>
            </section>
        </li>




        <li>
            <section id="Baseline">
                <div class="section_title">
                    <h2>
                        <strong>Retrieval Baseline</strong></h3>
                        <div class="section_body">
                            <p>In this project, we choose DenseNet-121 [9], the model gives the best performance among the TLL's proposed baseline, as our backbone. We first implement TLL Baseline proposed in the original TLL Paper and then we add BAM dataset by fine-tuning, LwF, and joint training to improved the pre-trained DenseNet-121's recognition ability. There are two baselines provided by the original TLL paper as mentioned in pervious sections and we will introduce both of them in the following. </p>
                            <p>Before starting, the training error of all the models we trained is in Table 0. Also, we give a sample architecture figure of joint-training model. All other methods have very similar architecture.</p>
                            <div style='text-align: center;font-size: 80%'>
							<img src="images/TLLReport/BaselineModel.PNG" style="width:60%;align-self:center" alt="" caption="aljda">
							<br />Figure 1: Example Baseline Architecture. (joint training)
						</div>
                            <div style='text-align: center;font-size: 80%'>
							<table class='table' style='text-align: left;font-size: 120%' >
                            <th>Method</th>
                            <th>ILSRVC2012 Before</th><th>ILSRVC2012 After</th><th>BAM Before</th>
                            <th>After</th>

                            <tr>
                            <td>BAM From Scratch</td>
                            <td>N/A</td><td>N/A</td><td>N/A</td><td>25.88% | 1.81%</td>
                            </tr>


                            <tr>
                            <td>BAM fine-tuned from ILSRVC2012</td>
                            <td>25.53% | 8.03%</td><td>N/A</td><td>83.28% | 36.34%</td><td>15.33% | 0.40%</td>
                            </tr>

                             <tr>
                            <td>BAM LwF from ILSRVC2012</td>
                            <td>25.53% | 8.03%</td><td>29.45%|10.03%</td><td>83.28% | 36.34%</td><td>13.30% | 0.28%</td>
                            </tr>

                             <tr>
                            <td>BAM joint-trained with ILSRVC2012</td>
                            <td>25.53% | 8.03%</td><td>31.64%|11.18%</td><td>83.28% | 36.34%</td><td>11.63% | 0.33%</td>
                            </tr>



                            </table>
                            <br /> Table 0: Error Rate (@1 | @5) of DenseNet-121 models we trained. Pytorch built-in DenseNet-121 were used for pre-trained models.
                            </div>
                            <p>The qualitive result of joint training can be visualized at <a href="http://tllvisualizer2018.azurewebsites.net"> TLL Retrieval Demo</a> for all the above models. (this one may requires log-in.)</p>



                            
                            <div>
                            <h3>General Recall Baseline</h3>
                            <p>Say, we have N pairs of TLL images¬, we divide them into a left set and a right set, such that for every pair, one image goes the left set and the other one goes the right set. In this way, the left set (size N) and the right set (size N) have a one-to-one mapping. When we perform the similarity retrieve, we are given a query image selected from the left set we want to find the its correct paired image from the right set. If the correct image is the first retrieval result, we call it find it in top 1, and etc.</p> 
                            <p>The original proposed method uses a deep neural net (ResNet or DenseNet) to extract features from input images by using the output of last activation layer. Then, for it computes the cosine distance for every possible pairs. The more similar two images are, the smaller the distance between their embedding is. Thus, for every query image, we will return a list of N images ordered by their distances from the query images (in the embedding space). If the ground truth paired image is found in the first to the k-th retrieval results, we call it “find in top k result”.</p>
                            <p>The original paper explores many deep neural net to extract the features and it turned out that DenseNet-121 will give the best result. Therefore, we will only reimplement this general recall by using DenseNet-121 and our following work will use DenseNet-121 as backbone too.</p>
                            <p>Here we report our self-implement retrieval based on DenseNet-121. For some reason, we couldn’t give the same result stated in the original paper. However, we decided not to spend too many time on it, because we believe our results make enough sense.</p>
                            
                            <h4> No-Duplicate Subset</h4>
                            Self-implemented Result on No_Dup Subset
                            <div style='text-align: center;font-size: 80%'>
							<table class='table' style='text-align: left;font-size: 120%' >
                            <th>Method</th>
                            <th>R@1</th><th>R@5</th><th>R@10</th>
                            <th>R@50</th><th>R@100</th>

                            <tr>
                            <td>DenseNet-121 (Reported Baseline)</td>
                            <td>5.80%</td><td>13.84%</td><td>16.90%</td><td>29.92%</td><td>38.89%</td>
                            </tr>


                            <tr>
                            <td>DenseNet-121 (Self-Implemented Baseline)</td>
                            <td>5.25%</td><td>12.36%</td><td>15.92%</td><td>29.43%</td><td>37.42%</td>
                            </tr>

                             <tr>
                            <td>DenseNet-121 FromScratch</td>
                            <td>1.42%</td><td>4.27%</td><td>5.91%</td><td>13.18%</td><td>19.26%</td>
                            </tr>

                              <tr>
                            <td>DenseNet-121 Finetune</td>
                            <td>3.39%</td><td>8.64%</td><td>12.04%</td><td>24.34%</td><td>34.24%</td>
                            </tr>


                            <tr>
                            <td>DenseNet-121 LwF</td>
                            <td>5.69%</td><td>12.91%</td><td>16.63%</td><td>30.09%</td><td>38.51%</td>
                            </tr>

                            <tr>
                            <td>DenseNet-121 Joint Training</td>
                            <td>6.18%</td><td>13.46%</td><td>17.34%</td><td>30.58%</td><td>40.43%</td>
                            </tr>
                            </table>
                            <br /> Table 1: Baseline on TLL Retrieval with No_Duplicate Subset. Model from scratch are training on BAM. Others are training from ILSRVC 2012 to BAM.
                            </div>

                            <h4> No-Face Subset</h4>
							<p>After exploring the no-duplicate TLL subset, we realize there are more than 80% images are about faces. Since we are more interested in the general non-trivial similarity, we decide to use the no-face subset provided TLL. As there is no baseline reported in the original paper. Here we construct our own baseline. </p>
							<p>Again, as realized TLL contains a large portion of cartoon images, we decide to leverage BAM, the art dataset, to help our model learn art features. Rather than use the original DenseNet-121 features to represent each images, we finetune/joint-training/LwF the pre-trained DenseNet-121 with BAM, and use the features output from the last activation layer of the new model to represent each images. We report the result below. Note, for comparison, we also report the retrieval results out of the original DenseNet-121 trained solely on ILSRVC2012 and BAM.
							<div style='text-align: center;font-size: 80%'>
							<table class='table' style='text-align: left;font-size: 120%' >
                            <th>Method</th>
                            <th>R@1</th><th>R@5</th><th>R@10</th>
                            <th>R@50</th><th>R@100</th><th>R@500</th>

                            <tr>
                            <td>DenseNet-121 (Baseline)</td>
                            <td>12.82%</td><td>25.65%</td><td>30.89%</td><td>45.38%</td><td>54.88%</td><td>79.49%</td>
                            </tr>

                             <tr>
                            <td>DenseNet-121 FromScratch</td>
                            <td>2.96%</td><td>8.14%</td><td>11.34%</td><td>22.75%</td><td>31.13%</td><td>64.67%</td>
                            </tr>

                              <tr>
                            <td>DenseNet-121 Finetune</td>
                            <td>6.72%</td><td>15.78%</td><td>20.22%</td><td>33.66%</td><td>42.73%</td><td>73.43%</td>
                            </tr>


                            <tr>
                            <td>DenseNet-121 LwF</td>
                            <td>12.70%</td><td>24.91%</td><td>29.34%</td><td>46.12%</td><td>55.49%</td><td>81.26%</td>
                            </tr>

                            <tr>
                            <td>DenseNet-121 Joint Training</td>
                            <td>12.89%</td><td>24.54%</td><td>30.27%</td><td>46.73%</td><td>54.25%</td><td>82.18%</td>
                            </tr>
                            </table>
                            <br /> Table 2: General Recall Baseline on TLL Retrieval with No_Face Subset. Model from scratch are training on BAM. Others are training from ILSRVC 2012 to BAM.
                            </div>
							TODO
							Give short explanation here
							</div>
							
							<div>
							<h3> Associative Recall Baseline</h3>
							<p>Another evaluation protocol introduced in the original paper is associative recall. Rather than try to find the most similar image from the ENTIRE right set as general recall does, here we are only interested in finding the correct answer from five images which are reasonably selected from the entire right set. The correct answer will be guaranteed to be included in the five options and the other distractors are randomly selected from the top K results of the general recall. Notice here, if K=N, then the distractors are equivalent to random selected from the entire right set.</p>
							<p>Here we report results on No_Face subset. In all below experiments, we selected distractors by ranking similarity from DenseNet-121 features distances (cosine).</p>
							<div style='text-align: center;font-size: 80%'>
							<table class='table' style='text-align: left;font-size: 120%' >
                            <th>Method</th>
                            <th>Top 5</th><th>Top 10</th><th>Top 20</th>
                            <th>Top 50</th><th>Top 100</th><th>top 200</th><th>top 500</th><th>All (Random)</th>

                            <tr>
                            <td>DenseNet-121 (Baseline)</td>
                            <td>12.85%</td><td>16.85%</td><td>21.58%</td><td>28.55%</td><td>32.86%</td><td>38.84%</td>
                            <td>49.26%</td>
                            <td>66.03%</td>
                            </tr>

                             <tr>
                            <td>DenseNet-121 from Scratch</td>
                            <td>21.33%</td><td>25.22%</td><td>24.35%</td><td>27.68%</td><td>32.06%</td><td>34.09%</td>
                            <td>37.98%</td>
                            <td>47.35%</td>
                            </tr>

                             <tr>
                            <td>DenseNet-121 Fine-tune</td>
                            <td>19.17%</td><td>25.71%</td><td>26.63%</td><td>30.52%</td><td>34.40%</td><td>40.38%</td>
                            <td>46.36%</td>
                            <td>57.83%</td>
                            </tr>

                             <tr>
                            <td>DenseNet-121 LwF</td>
                            <td>13.81%</td><td>19.17%</td><td>22.38%</td><td>29.28%</td><td>35.45%</td><td>41.25%</td>
                            <td>51.48%</td>
                            <td>68.31%</td>
                            </tr>

                             <tr>
                            <td>DenseNet-121 Joint Training</td>
                            <td>16.34%</td><td>21.09%</td><td>24.48%</td>
                            <td>31.07%</td><td>35.94%</td><td>42.79%</td>
                            <td>51.48%</td>
                            <td>68.31%</td>
                            </tr>


                            </table>
                            <br /> Table 3: Associative Recall Baseline on TLL Retrieval with No_Face Subset. Model from scratch are training on BAM. Others are training from ILSRVC 2012 to BAM.
                            </div>
                            <p>Note, here is a serious problem with associate recall when the distactor is not randomly selected from the entire dataset. As how associative recall is defined, if the distracted selected from top K similar images from the DenseNet-121 features, it is naturally hard for the models trained on top of DenseNet-121 to perform retrieval. Thus, the more different from DenseNet-121, the models are, the higher recall rate it can reach, when K is small. This is why the model trained from scratch of BAM give a very good recall number when K=5, but other models beat it when K gets larger. </p>
                            Thus, it is reasonable to believe that only distractors randomly selected from the dataset(K=N) could give a fair evaluation of the performance of models. Thus, in the following we will only use associative recall with K=N (i.e., distractors are toally randomly selected). 
							</div>

							<div>
							<h3>K-Mean Baseline</h3>
							<p>TBD</p>
							</div>
                		</div>
            </section>
        </li>
        <li>


            <section id="Conditional Retrieval">
                <div class="section_title">
                    <h2><strong>Retrieval Based on Condtional Concepts</strong></h2>
                    <div class="section_body">

                        <p>From observation, we notice that the features extracted directly from deep neural net can represent a general sense of the image,  so its retrieval would be performed only based on general sense emphasized on training dataset. For example, given a query image, that a girl wearing a rainbow clothing walking on a beach, it will return the pictures with girls walking on the beach, rather than images with rainbow. Although this retrieval still finds images with appreciable similarity, they are not as specialized to enable retrieve eimages based non-general criteria. Therefore, we want to find a way that enable us retrieve images with emphasis on different visual concepts, such as color, shape, ensemble faces, contour and etc.</p>
                        <p>We run a few round of trials to train deep nets, that are able to separate different visual concepts and retrieve images based on each concept.</p>

                        <div>
                        <h3>Evaluation Metrics</h3>
                        <p>Before we go through our trials, I am going to introduce how we combine the retrival results given from each concept branch and get the final recall (i.e., how to decide which branch (which concepts/which kind of similarity) to refer to for every image.) </p>
                        <h4>Naive Combination (White box)</h4>
                        <p>This evaluation will give us the upper bound of model performances. For every given image, if there are M conditional branch such that every branch learns a different visual concepts, we do M retrieval based on features extracted from the M branchs. If the true paired image is found at k-th retrieval results of any of branchs, we use the smallest k as the combined result. For example, if there are four branchs, and the restrieval results for image with index 2 are (i=2,k=1), (i=2,k=3), (i=2,k=5), and (i=2,k=801) respectively, the combined result would be k=1. 
                        <h4>Auto Combination (Black box)</h4>
                        <p>Since using ground truth to decide which branch to refer to for every image is kind of cheating, here we introduce an automatic way to decide which retrieval branch to follow. By this auto combination method, we will choose the branch that gives the smallest distances. Recall, we retrieve similarity based on the cosine distance of the embeddings of two images. Therefore, for every image and every branch, we will get a distance. Thus, the conbined result will be result out of the m-th branch if the m-branch gives the smallest embedding distances of the query image and the ground truth image. 
                        <p>Consider a top 1 retrieval result example. If there are four branchs, and the best retrieval results (result with smallest distance) for image with index 3 of each branch and their cosine distances are (i=1,d=0.4), (i=2,d=0.5),(i=3,d=0.02) and (i=801,d=0.003) respectively, the combined result would be i=2, (which means that the top 1 retrieval is wrong.) </p> 
                        </div>

						<div>
						<h3>First Trial: Conditioning by training separated models</h3>
						<p>First, we train two separate models to represent different concepts. The first model is DenseNet-121 joint training with ILSRVC 2012 BAM to represent general features. The second models is DenseNet-121 joint training with ILSRVC 2012, BAM, Gaussian blurred ILSRVC 2012 and Gaussian blurred BAM to represent the shape/color/more abstract general sense of image. 
						<p> Then we found that the embedding from the blurred DenseNet does respond to different visual concepts especially in sense of contour. This may be caused the blur makes there less fine-grained details in the image, so the contour and shape was emphasized and learnt better. </p>
						<div style='text-align: center;font-size: 80%'>
							<table class='table' style='text-align: left;font-size: 120%' >
                            <th>Method</th>
                            <th>R@1</th><th>R@5</th><th>R@10</th>
                            <th>R@50</th><th>R@100</th><th>R@500</th>

                            <tr>
                            <td>DenseNet-121 (Baseline)</td>
                            <td>12.82%</td><td>25.65%</td><td>30.89%</td><td>45.38%</td><td>54.88%</td><td>79.49%</td>
                            </tr>
                            <tr>
                            <td>DenseNet-121 Joint Training</td>
                            <td>12.89%</td><td>24.54%</td><td>30.27%</td><td>46.73%</td><td>54.25%</td><td>82.18%</td>
                            </tr>
                            <tr>
                            <td>DenseNet-121 Joint Training (with Blur)</td>
                            <td>11.28%</td><td>21.51%</td><td>26.57%</td><td>43.96%</td><td>54.25%</td><td>82.18%</td>
                            </tr>
                            <tr>
                            <td>DenseNet-121 Naive Combined</td>
                            <td>16.21%</td><td>29.84%</td><td>35.88%</td><td>53.45%</td><td>63.50%</td><td>88.84%</td>
                            </tr>
                            </table>
                            <br /> Table 4: General Recall of conditional method 1. (Blurred).
                            </div>
                            From the native combination result, we can see these two models are learning to represent features with different emphasis. We also provide some qualitive result in Figure 2.
                            <div style='text-align: center;font-size: 80%'>
							<img src="images/TLLReport/BlurPosEx1.PNG" style="width:60%;align-self:center" alt="" caption="aljda">
							<img src="images/TLLReport/BlurPosEx2.PNG" style="width:60%;align-self:center" alt="" caption="aljda">
							<br />Figure 2: Left column is the query image and its ground truth pair. Middle column is ordered retrieval results given by joint training model. Right column is the retrieval result  ordered retrieval results given the blurred joint training result. From the Top 2 examples, it shows that the blurred model perform better when the similarity is infering from the images as whole. And the bottom two examples show that the the non-blurred model works better when the similarity has more emphasis on the details (e.g. textures).
						</div>

						</div>

						<div>
						<h3>Second Trial: Conditioning from Conditional Network</h3>
						<p>Inspired by CITE and CSN, we tried to train a conditional neural network, in a hope that the neural net can learn how to decompose the raw embedding out of DenseNet-121 into several conditional embedding with weights assigned to each condtional embedding itself. We hope each embedding could learn a different visual concept, which we can perform retrieval based on.</p>
						<p>Recall, our goal is to train the embedding encoded by different conditional branch could representing different concepts. Thus, though we train our neural net by performing two tasks classification and similarity triple comparison, the only thing we want is the embedding output from each conditional branch. The reason why we have to train the neural net by performing other tasks rather than similarity retrieval itself is, the TLL dataset is not large enough for us to train such a network. Thus, t is not realistic to train it explicitly like what is done in Conditional Similarity Network. Thus, we designed our network following the idea of CITE (a weight branch and a branch for conditional branchs), as shown in Figure 4 and 5.</p>
						
						<h4>Training by Classification Task</h4>
						<p>Alternatively, the first training we tried is to perform a classification task. We use proposed network to joint-train data from ILSRVC 2012 and BAM and to enable the conditional layers to learn different concepts. Model is shown in Figure 3.
						Here we report experiment result by setting # of conditional branch M = 4. quantatitive result is below. The qualitive result can be visualized at <a href="http://tllvisualizer2018.azurewebsites.net/k_subspace"> TLL Retrieval Demo</a>.
						
						<div style='text-align: center;font-size: 80%'>
						<img src="images/TLLReport/ConditionalClassifier1.PNG" style="width:60%;align-self:center" alt="" caption="aljda">
						<br />Figure 4: Network Architecture of Condtional Retrieval Training on Classification Task.
						</div>

						<div style='text-align: center;font-size: 80%'>
							<table class='table' style='text-align: left;font-size: 120%' >
                            <th>Method</th>
                            <th>R@1</th><th>R@5</th><th>R@10</th>
                            <th>R@50</th><th>R@100</th><th>R@500</th><th></th><th>Associative Recall</th>

                            <tr>
                            <td>DenseNet-121 (Baseline)</td>
                            <td>12.82%</td><td>25.65%</td><td>30.89%</td><td>45.38%</td><td>54.88%</td><td>79.49%</td>
                            <td></td><td>66.03%</td>
                            </tr>
                            <tr>
                            <td>DenseNet-121 Joint Training</td>
                            <td>12.89%</td><td>24.54%</td><td>30.27%</td><td>46.73%</td><td>54.25%</td><td>82.18%</td>
                            <td></td><td>68.31%</td>
                            </tr>
                            
                            <tr>
                            <td>Conditional M=4 (classification) - Naive Combination</td>
                            <td>14.30%</td><td>28.00%</td><td>34.90%</td><td>52.22%</td><td>61.41%</td><td>87.73%</td>
                            <td></td><td>76.33%</td>
                            </tr>
                            <tr>
                            <td>Conditional M=4 (classification) - Auto Combination</td>
                            <td>11.28%</td><td>21.64%</td><td>28.00%</td><td>46.30%</td><td>54.87%</td><td>81.63%</td>
                            <td></td><td>69.36%</td>
                            </tr>
                            <tr>
                            <td>Conditional M=4 (classification) - Shared</td>
                            <td>12.95%</td><td>24.91%</td><td>30.58%</td><td>47.78%</td><td>56.84%</td><td>82.57%</td>
                            <td></td><td>68.13%</td>
                            </tr>
                            <tr>
                            <td>Conditional M=4 (classification) - Dot Product</td>
                            <td>11.10%</td><td>21.76%</td><td>27.68%</td><td>45.01%</td><td>54.93%</td><td>81.63%</td>
                            <td></td><td>68.68%</td>
                            </tr>

                            </table>
                            Table 5: General Recall and Associative Recall (Random) of Conditional Network training by classification task. "Shared" are result computed by using embedding output directly from DenseNet-121 before inputting to the the division. "Dot Product" are computed by embedding after dot product the weight and condtional embeddings.
                            </div>
                            <p>From the quantatitive result, we can see the proposed method can give slightly better result by both using the conditional embeddings (rows 3 and 4 in table 5) or just use the shared or producted embedding output by the neural net itself (rows 5 and 6 in table 5). </p>
                        
                        <h4>Training by Similarity Triplet</h4>
                        <p>As the original Conditional Similarity Network and CITE are trained as similarity network and triplet are used there. We want to train our proposed network with similarity triplet to see whether it could improve the performance. We adjust our architecture as shown in Figure 5. </p>
                        <p>In our experiment, we use BAM and ILSRVC dataset. For each dataset, we sample triplet as (query image, random image from the same category, and random image from another category). We report the experiment results with M=4. Same as earlier, the quantatitive result is below and the qualitive result can be visualized at <a href="http://tllvisualizer2018.azurewebsites.net/k_subspace"> TLL Retrieval Demo</a>.</p>

						<div style='text-align: center;font-size: 80%'>
						<img src="images/TLLReport/ConditionalSimi1.PNG" style="width:60%;align-self:center" alt="" caption="aljda">
						<br />Figure 5:Network Architecture of Condtional Retrieval Training on Similarity Task. Note the architecture are shared by the triplet.
						</div>
						<div style='text-align: center;font-size: 80%'>
							<table class='table' style='text-align: left;font-size: 120%' >
                            <th>Method</th>
                            <th>R@1</th><th>R@5</th><th>R@10</th>
                            <th>R@50</th><th>R@100</th><th>R@500</th><th></th><th>Associative Recall</th>

                            <tr>
                            <td>DenseNet-121 (Baseline)</td>
                            <td>12.82%</td><td>25.65%</td><td>30.89%</td><td>45.38%</td><td>54.88%</td><td>79.49%</td>
                            <td></td><td>66.03%</td>
                            </tr>
                            <tr>
                            <td>DenseNet-121 Joint Training</td>
                            <td>12.89%</td><td>24.54%</td><td>30.27%</td><td>46.73%</td><td>54.25%</td><td>82.18%</td>
                            <td></td><td>68.31%</td>
                            </tr>
                            
                            <tr>
                            <td>Conditional M=4 (classification) - Naive Combination</td>
                            <td>14.30%</td><td>28.00%</td><td>34.90%</td><td>52.22%</td><td>61.41%</td><td>87.73%</td>
                            <td></td><td>76.33%</td>
                            </tr>
                            <tr>
                            <td>Conditional M=4 (classification) - Auto Combination</td>
                            <td>11.28%</td><td>21.64%</td><td>28.00%</td><td>46.30%</td><td>54.87%</td><td>81.63%</td>
                            <td></td><td>69.36%</td>
                            </tr>
                            <tr>
                            <td>Conditional M=4 (similarity triplet) - Naive Combination</td>
                            <td>7.64%</td><td>16.21%</td><td>20.10%</td><td>34.03%</td><td>43.83%</td><td>74.86%</td>
                            <td></td><td>60.67%</td>
                            </tr>
                            <tr>
                            <td>Conditional M=4 (similarity triplet) - Auto Combination</td>
                            <td>6.41%</td><td>13.01%</td><td>16.46%</td><td>28.00%</td><td>36.25%</td><td>66.58%</td>
                            <td></td><td>52.03%</td>
                            </tr>


                            </table>
                            Table 6: General Recall and Associative Recall (Random) of Conditional Network training by classification task and similarity triplet task.
                            </div>
                            <p>Apparently, the proposed network trained on similarity triplet task is not as good as trained with classification task. This could be caused by the way how the triplet is simpled. The sampling way enforces the network to learn the between-class differences, so the network will be more sensitive to within-class similarity. However, the kind of simiarlity in TLL that we want to detect is not in-class similarity but a concept-based, more abstract and general similarity, which could be a reason of the failure.</p>
                            <br />
                            <p>Overall, from the qualitive result, we can see the conditional branchs are not learning separated concepts and they behave very similar. Thus, we want to find a way to train the proposed network to learn separated concepts, which is what next section is about.
						</div>

						<div>
						<h3>Third Trial: Adjustment with Network to Learn More Independent Concepts</h3>
						<p>By observing the qualitative result from the previous experiments, we realize that the concept that each conditional learned are not separated. If you consider them as vector bases, they are not perpendicular. However, we want them to be as independent as possible, so that we can leverage their separation and combination of them to help us find more customized simlarity. Therefore, in this section, we are going to explore how we can train each branch more independent.</p>
						<p>Before everything, we first adjust our proposed network a bit to enable it deal with more complexity. Basically, we add several more fully-connected layers and we normalize the learned embeddings of the conditional layers as we will abstractize them as "base vectors" from now. As tested, training with classification task would fit better to our task, we will train with classification task for this round of experiments as well. The new proposed NN architecture is shown in Figure 6.</p>

						<div style='text-align: center;font-size: 80%'>
						<img src="images/TLLReport/ConditionalClassifier2.PNG" style="width:60%;align-self:center" 
						alt="" caption="aljda">
						<br />Figure 6: New Conditional Network Architecture.
						</div>

						<h4>More Conditional Branchs (Larger M)</h4>
						<p>In the previous experiments, the number of conditional branchs was kept as four. However, it could be very difficult to completely decompose a complex 1024x1 embedding into 4 conditional branch. This is to say, each of the four conditional embeddings can still be a combination of many basic visual concepts, which makes four of them gives similarly retrieval results. Thus, here we are going to try different M, and report the results of M=4,16,64.</p>

						<h4>Perpendicular Loss</h4>
						<p>Another way we tried to enforce each of the M conditional branch to learn different concepts is adding an additional loss which punishes the non-perpendicular conditional pairs as ||<strong>a</strong> \dot <strong>b</strong> || for every conditional component pair <strong>a</strong> and <strong>b</strong>. The running time of this loss grows expoentially, so the training is slow. Also, this makes it very hard to train while keep the ILSRVC2012's accuracy, my best case, which is reported below, is top1 error rate around 60%. </p>

						<p>Here we report the results of the methods proposed in this seciton.</p>

						<div style='text-align: center;font-size: 80%'>
							<table class='table' style='text-align: left;font-size: 120%' >
                            <th>Method</th>
                            <th>R@1</th><th>R@5</th><th>R@10</th>
                            <th>R@50</th><th>R@100</th><th>R@500</th>

                            <tr>
                            <td>DenseNet-121 (Baseline)</td>
                            <td>12.82%</td><td>25.65%</td><td>30.89%</td><td>45.38%</td><td>54.88%</td><td>79.49%</td>
                            
                            </tr>
                            <tr>
                            <td>DenseNet-121 Joint Training</td>
                            <td>12.89%</td><td>24.54%</td><td>30.27%</td><td>46.73%</td><td>54.25%</td><td>82.18%</td>
                            
                            </tr>
                            
                            <tr>
                            <td>Conditional M=4 (classification) - Naive Combination</td>
                            <td>14.30%</td><td>28.00%</td><td>34.90%</td><td>52.22%</td><td>61.41%</td><td>87.73%</td>
                            
                            </tr>

                            <tr>
                            <td>Conditional M=16 (classification) - Naive Combination</td>
                            <td>21.21%</td><td>38.16%</td><td>44.94%</td><td>64.98%</td><td>74.78%</td><td>96.48%</td>
                            
                            </tr>

                            <tr>
                            <td>Conditional M=64 (classification) - Naive Combination</td>
                            <td>28.79%</td><td>47.72%</td><td>53.82%</td><td>75.89%</td><td>84.09%</td><td>98.80%</td>
                            
                            </tr>

                            <tr>
                            <td>Conditional M=4 Perpendicular (classification) - Naive Combination</td>
                            <td>12.08%</td><td>22.81%</td><td>28.11%</td><td>44.76%</td><td>55.12%</td><td>84.77%</td>
                            
                            </tr>


                            </table>
                            Table 7: General Recall with Naive Conbination by classification task of all trials.
                            </div>
						</div>
						<div style='text-align: center;font-size: 80%'>
							<table class='table' style='text-align: left;font-size: 120%' >
                            <th>Method</th>
                            <th>R@1</th><th>R@5</th><th>R@10</th>
                            <th>R@50</th><th>R@100</th><th>R@500</th>

                            <tr>
                            <td>DenseNet-121 (Baseline)</td>
                            <td>12.82%</td><td>25.65%</td><td>30.89%</td><td>45.38%</td><td>54.88%</td><td>79.49%</td>
                            
                            </tr>
                            <tr>
                            <td>DenseNet-121 Joint Training</td>
                            <td>12.89%</td><td>24.54%</td><td>30.27%</td><td>46.73%</td><td>54.25%</td><td>82.18%</td>
                           
                            </tr>
                            
                            <tr>
                            <td>Conditional M=4 (classification) - Auto Combination</td>
                            <td>11.28%</td><td>21.64%</td><td>28.00%</td><td>46.30%</td><td>54.87%</td><td>81.63%</td>
                            
                            </tr>

                             <tr>
                            <td>Conditional M=16 (classification) - Auto Combination</td>
                            <td>8.32%</td><td>18.56%</td><td>22.01%</td><td>35.14%</td><td>44.08%</td><td>73.55%</td>
                            </tr>

                             <tr>
                            <td>Conditional M=64 (classification) - Auto Combination</td>
                            <td>6.17%</td><td>15.60%</td><td>20.47%</td><td>36.87%</td><td>44.51%</td><td>73.92%</td>
                            </tr>

                             <tr>
                            <td>Conditional M=4 Perpendicular (classification) - Auto Combination</td>
                            <td>9.43%</td><td>16.15%</td><td>20.10%</td><td>32.61%</td><td>40.38%</td><td>68.93%</td>
                            </tr>

                            </table>
                            Table 8: General Recall with Naive Conbination by classification task of all trials.n task and similarity triplet task.
                            </div>
                            <p>The qualitive result can be visualized at <a href="http://tllvisualizer2018.azurewebsites.net/k_subspace"> TLL Retrieval Demo</a> for all the above models.</p>
                            <p>Here, we introduce a new metrics to evaluate how separate the conditional branchs are. Known the more similar two conditional branchs are, they will give more similar embeddings and thus the more overlap will be with their retrieval result. Thus, we use the following formula to measure the overlap of the retrieval results, which can indicate how different each pairs of conditional branch are. The Average Overlap is defined as sum_i(# overlaps over retrieval results out of ALL conditional branchs in top K results)/N, where N is the dataset size. </p>


                            <div style='text-align: center;font-size: 80%'>
							<table class='table' style='text-align: left;font-size: 120%' >
                            <th>Method</th>
                            <th>top 5</th><th>top 10</th><th>top 20</th>
                            <th>top 50</th><th>top 100</th><th> top 500</th>
                            
                            <tr>
                            <td>Conditional M=4 (classification) </td>
                            <td>2.6</td><td>5.4</td><td>11.26</td><td>29.25</td><td>60.78</td><td>343.36</td>
                            
                            </tr>

                            <tr>
                            <td>Conditional M=16 (classification)</td>
                            <td>1.48</td><td>3.19</td><td>6.76</td><td>17.87</td><td>38.49</td><td>256.04</td>
                            
                            </tr>

                            <tr>
                            <td>Conditional M=64 (classification)</td>
                            <td>0.19</td><td>0.34</td><td>0.65</td><td>1.61</td><td>3.52</td><td>32.14</td>
                            
                            </tr>

                            <tr>
                            <td>Conditional M=4 Perpendicular (classification)</td>
                            <td>1.01</td><td>2.2</td><td>4.74</td><td>13.27</td><td>29.25</td><td>210.05</td>
                            
                            </tr>


                            </table>
                            Table 9: Average Overlap from all conditional branchs in Top k retrieval results.
                            </div>
                            <p>From the above results, we can see:
                            <ol>
                            <li> None of the methods proposed in this section beat the baseline;</li>
                            <li> the more conditional branchs there are, the neural network will learn more separated concepts, but also more difficult to decide which branch to refer to when output the final retrieval results; and </li>
                            <li> the perpendicular loss has ptential to force each branch to learn separated concepts (as the average overlap halfed after adding perpendicular loss). However, it is very hard to train it achieve the same accuracy as without this loss, which is also the reason why the retrieval recall is not as high as the other method. (During the training, the classification accuracy converges around 40% for ImageNet and 90% for BAM.) It may give better and promising result if we can find an appropriate way to train it. However, the time complexity is still a pain. </li>
                            </ol>
                            </p>
                    
                    </div>
                </div>
            </section>


        </li>
        <li>


            <section id="discussion">
                <div class="section_title">
                    <h2><strong>Discussion</strong></h2>
                    <div class="section_body">
						<div>
							<h3>TLL Dataset</h3>
							<p>Our initial goal is to get a better performance on the TLL retrieval. However, with exploring the TLL dataset, we realize that there are some dataset issues which could make the task hard. Though the duplicates are supposed to be removed, there are still some duplicates remaining in the cleaned subset which could hurt the accuracy. Also, even without the duplicates, there could be more than one image similar to a query image by human judgement, but there is only one groundtruth. Plus, this dataset is too small to be used as training set in many cases, so we have to face the dataset bias problem.</p>
						</div>
						<div>
							<h3>Potential Solution for This TLL Retrieval</h3>
							<p>As we exploring how to train a neural network to learn separated visual concepts from a single image, we found the embedding semantic decomposition is very interesting. Inspired by the IBD paper, IBD could potential not only be applied to nerual network visualization but also other tasks like this image retrievals. We could compute embedding projections on the concepts that we are interested, find the similarity based on the selected concepts. In this way, we can get a concept-specialized similarity score. Besides, we can also use the low level concepts from BroDen to train the concept branchs as what has been done in the CSN work.</p>
						</div>
						<div>
							<h3>Potential Topic Shift</h3>
							<p>The topic of Interpretable Basis Decomposition itself or just the semantic decomposition in latent space seems very interesting and not well explored yet. There are still many interesting questions with IBD, such as the independences of the visual concepts conponents, the large residuals of the decomposition or the representing ability of visual concepts conponent encoding. 
							Besides, the semantic decompostion in latent spaces might have more application than just visualization. It may be generalized to image synthesis, multi-task training or any multiple concepts involved task?</p>
						</div>

                    </div>
            </section>

            <section id="references">
                <div class="section_title">
                    <h2><strong>References</strong></h2>
                    <div class="section_body">
                    <p>[1] Rosenfeld, Amir, Markus D. Solbach, and John K. Tsotsos. "Totally Looks Like-How Humans Compare, Compared to Machines." arXiv preprint arXiv:1803.01485 (2018).</p>
                    <p>[2] Veit, Andreas, Serge J. Belongie, and Theofanis Karaletsos. "Conditional Similarity Networks." In CVPR, vol. 1, no. 2, p. 3. 2017.</p>
                    <p>[3] Plummer, Bryan A., Paige Kordas, M. Hadi Kiapour, Shuai Zheng, Robinson Piramuthu, and Svetlana Lazebnik. "Conditional Image-Text Embedding Networks." arXiv preprint arXiv:1711.08389 (2017).</p>
                    <p>[4] Zhou, Bolei, Yiyou Sun, David Bau, and Antonio Torralba. "Interpretable basis decomposition for visual explanation." In Proceedings of the European Conference on Computer Vision (ECCV), pp. 119-134. 2018.</p>
                    <p>[5] Wilber, Michael J., Chen Fang, Hailin Jin, Aaron Hertzmann, John Collomosse, and Serge J. Belongie. "BAM! The Behance Artistic Media Dataset for Recognition Beyond Photography." In ICCV, pp. 1211-1220. 2017.</p>
                    <p>[6] Li, Zhizhong, and Derek Hoiem. "Learning without forgetting." IEEE Transactions on Pattern Analysis and Machine Intelligence 40, no. 12 (2018): 2935-2947.</p>
                    <p>[7] Bau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. "Network dissection: Quantifying interpretability of deep visual representations." arXiv preprint arXiv:1704.05796 (2017).</p>
                    <p>[8] Russakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang et al. "Imagenet large scale visual recognition challenge." International Journal of Computer Vision 115, no. 3 (2015): 211-252.</p>
                    <p>[9]Huang, Gao, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. "Densely connected convolutional networks." In CVPR, vol. 1, no. 2, p. 3. 2017.</p>
                    </div>
                    </div>
            </section>

        </li>


           
    </ul>


    </div>
    <!---->

    <footer>
        <p>&copy; Copyright 2018 - Aiyu Cui</p>
    </footer>
</body>
</html>